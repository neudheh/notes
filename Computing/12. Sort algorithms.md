## Bubble sort
### Non-Optimized
1. Compare the first and second values, and if the first value is larger than the second value, swap them
2. Compare the second and third values, and if the second value is larger than the third value, swap them
3. Keep comparing adjacent values, swapping them if necessary, until the last two values in the list have been processed
4. When we complete the first pass though an entire array, the largest value is in the correct position; but other values may or may not be in the correct order
5. We need to work through the array again and again with each pass having one less value to compare
```pseudo
// Compares up to 2nd last element (assumes list index starts from 0)
FOR passes <- 0 TO length of data - 2
	//Compares elements before those already sorted
	FOR j <- 0  TO length of data - 2 - passes
		IF data[j] > data[j + 1]
			THEN 
				// If the current value is more than the next value, swap
				data[j], data[j + 1] <- data[j + 1], data[j]
		ENDIF
	ENDFOR j
ENDFOR passes
```
- The values to be sorted may already be in the correct order before the outer loop has been through all its iterations
- After the third pass, the values are in the correct order, but the algorithm will continue to run
- This means we are making comparisons when no further swaps need to be made
![[room to optimize.jpg]]
### Optimized
- If we have gone through one pass without swapping any values, this means that the values must be in the correct order
- We can use a variable `swapped` to store whether a swap has taken place
- When we swap a pair of values, we set `swapped` to `True`, indicating that the list was not swapped
- If `swapped` is `False` at the end of one pass through the list, this means that none of the values are swapped, and the list must be sorted and we can exit the algorithm
- This allows us to cut down on any unnecessary comparisons and passes, optimizing the algorithm
```pseduo
swapped <- TRUE
passes <- 0
WHILE swapped DO
	swapped <- FALSE
	FOR j <- 0 TO length of length of data - 2 - passes
		IF data[j] > data[j + 1]
			THEN
				data[j], data[j + 1] = data[j + 1], data[j]
				swapped <- True
		ENDIF
	ENDFOR
	passes <- passes + 1
ENDWHILE
```

- Best case time complexity is $\Omega (n)$, when we are given a sorted list, we only need one pass of $n - 1$ comparisons on the values in the list to check that it is sorted
- Worst case time complexity is $O(n^2)$ as the non-optimized bubble sort algorithm will need to make $(n-1) \times (n-2)$ comparisons to complete the sort
## Insertion sort
- Insertion sort divides the array into two parts, sorted and unsorted 
- Unsorted elements are inserted one by one into their sorted section
1. Make the first element in the array a part of the sorted list
2. Consider the next element in the unsorted array, if it is smaller than the first, we swap them
3. Consider the third element in the array; we swap it leftwards until it is in its proper with the first two elements
4. Repeated until the whole array is sorted
```pseudo
FOR i <- 1 to length of data - 1
	curr <- data[i]
	position <- i
	WHILE position > 0 and curr < data[position - 1] DO
		data[position] <- data[position - 1]
		position <- position - 1
	ENDWHILE
	data[positon] <- curr
ENDFOR i
```
### Time complexity
- The outer for loop in insertion sort always iterates $n-1$ times
- The inner while loop will make $n-1$ comparisons in the best case if all values are in the correct order
- The inner while loop will make $1+2+3+...+(n-1)$ comparisons in the worst case
- Best case time complexity of $\Omega(n)$
- Worst case time complexity of $O(n^2)$
## Merge sort
- Merge sort is a divide and conquer algorithm that recursively uses two functions
- One splits the array into half, and one that merges both halves, producing a sorted array
### Merge function
- If we have two sorted lists in ascending order, `List1` and `List2`, we compare the first item in both lists
- If the first item in `List1` is smaller than that in `List2`, the first item is removed from `List1` and added to a new list, and vice versa
- This comparison is repeated until either `List1` or `List2` becomes empty, and the remaining items on the list are added to the new list and the merge is complete
```pseudo
FUNCTION merge (list1: ARRAY, list2: ARRAY) RETURNS ARRAY
	i <- 0
	j <- 0
	new_list <- []
	WHILE i < length of list1 and j < length of list2
		IF list1[i] < list2[j]
			THEN 
				APPEND list1[i] TO new_list
				i <- i + 1
			ELSE
				APPEND list2[j] TO new_list
				j <- j + 1
		ENDIF
	ENDWHILE

	//if list1 or list2 has remaining items, add it to the new list
	IF i < length of list1:
		THEN
			new_list <- new_list + list1[i:]
		ELSE
			new_list <- new_list + list2[j:]
	ENDIF
	RETURN new_list
ENDFUNCTION
```
### Merge sort function
- This function recursively divides an unsorted array into subarrays, until each contains one element
- Then, it merges the subarrays into a sorted subarray, until there is only one sorted array
```pseudo
FUNCTION merge_sort(A: ARRAY) RETURNS ARRAY
	IF length of A > 1
		THEN
			mid <- length of A DIV 2
			list1 <- A[mid:]
			list2 <- A[:mid]
			sorted_list1 <- merge_sort(list1)
			sorted_list2 <- merge_sort(list2)
			RETURN merge(sorted_list1, sorted_list2)
	ENDIF
	RETURN A
ENDFUNCTION
```
### Time complexity
- The time complexity of dividing the arrays is $O( \log n )$, and the time complexity of merging the subarrays is $O(n)$
- Thus, merge sort has a time complexity of $O(n \log n)$
- The time complexity for best and worst cases is the same, as merge sort always divides the arrays into two halves and takes linear time to merge both halves
## Quicksort
### Out-of-place
- Quicksort first chooses a pivot element, which can be any element in the array
- The algorithm then partitions the array around the pivot
- Every element lesser than the pivot is place in a `less_than` array, and every element greater than or equal to the pivot is place in a `greater_equal` array
- Eventually, those elements are combined in order by first inserting the elements of `less_than`, then those that are equal, finally those in `greater_equal`
```pseudo
FUNCTION quicksort(A: ARRAY) RETURNS ARRAY
	IF NOT A // A is empty
		THEN
			RETURN []
	pivot <- A[0] // Taking the first element as the pivot
	less_than <- array of all the elements in A less than the pivot
	equal <- pivot
	greater_equal <- array of all the elements in A greater than or equal to the pivot
	RETURN quicksort(less_than) + [equal] + quicksort(greater_equal)
ENDFUNCTION
```
- Works well if all elements are unique
- If they are not, the order of occurrence of two same elements may be altered, making the algorithm an unstable sorting algorithm
### In-place
- Recursion on large data sets can cause the computer to run out of memory, causing a runtime error
- An in-place quicksort algorithm can be used instead, which uses lesser memory
1. The divide step is performed by scanning the array and using the left index, `l`, which advances forwards, and a right index, `r`, which advances backwards
2. A swap is performed when `l` is at an element larger than the pivot and `r` is at an element smaller than the pivot
3. `l` continues to advance forwards and `r` continues to advance backwards
4. When `l` is greater than `r`, `l` swaps with the pivot, completing the divide step of in place quick swap
5. The algorithm then recurs on these two sub arrays
![[in place quicksort 1.png]]
![[in place quicksort 2.png]]
```pseudo
PROCEDURE inplace_quicksort(A: ARRAY, first: INTEGER, last: INTEGER)
	IF first < last
		THEN
			pivot <- A[last]
			l <- first
			r <- last - 1
			WHILE l <= r DO
				WHILE l <= r AND A[l] < pivot DO
					l <- l + 1
				ENDWHILE
				WHILE l <= r AND A[r] > pivot DO
					r <- r - 1
				ENDWHILE
				IF l <= r
					THEN
						A[l], A[r] <- A[r], A[l]
						l, r <- l + 1, r - 1
				ENDIF
			ENDWHILE
			A[left], A[last] = A[last], A[left]
			inplace_quicksort(A, first, l - 1)
			inplace_quicksort(A,l + 1, last)
	ENDIF
ENDPROCEDURE
```
### Time complexity
- Similar to merge sort, quicksort has a time complexity of $\Omega(n \log n)$ , 
- In the worst case, the pivot is always  the smallest or largest element, creating partitions of size $n-1$, calling recursive calls $n-1$ times, leading to a time complexity of $O(n^2)$ 
## Merge sort vs Quicksort
- Quicksort is an unstable sorting technique. It might change the occurrence of two similar elements in the array while sorting.
- Merge sort is a stable algorithm. It doesnâ€™t change the occurrence of similar elements and preserves the order in which these elements appeared in the original array
- Instead of adding elements greater than and equal to the pivot into the `greater_equal` array, elements equal to the pivot can be stored in an `equal` array and elements greater than pivot can be stored in a `greater` array. 
- This will preserve the original order of occurrence of the elements.
```Pseudo
pivot <- element in the middle of the array
less <- an array of all elements in A less than pivot
equal <- an array of all elements in A equal to pivot
greater <- an array of all elements in A greater than pivot
```
- In-place quicksort does not need additional memory space to perform sorting. 
- Merge sort requires a temporary array to merge the sorted arrays.
- Hence, merge sort needs more memory space compared to in-place quicksort.